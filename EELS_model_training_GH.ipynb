{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEL_model_training-github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOtOHKPi/umytLCiXutCT9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patecm/rapidEELS/blob/main/EELS_model_training_GH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDZ5J2yhIB3a",
        "outputId": "bac17eff-baa6-47c1-dd90-fb99f6fc503f"
      },
      "source": [
        "#%% === USER PREFERENCES\n",
        "# --- Specify location of EELS files\n",
        "path_files = '/tmp/eels/ex situ SFO 2-5-20'\n",
        "path_dark = '/tmp/eels/bkg_sub.dm4'\n",
        "\n",
        "# -- General Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import os\n",
        "from os import listdir, mkdir\n",
        "from os.path import isfile, join, exists\n",
        "from datetime import datetime, date\n",
        "from pytz import timezone\n",
        "#import progressbar\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
        "from skimage.measure import block_reduce\n",
        "\n",
        "# -- Import previously installed packaged\n",
        "# LMFit\n",
        "from lmfit.models import GaussianModel, PowerLawModel\n",
        "# Hyperspy - for reading dm4 files\n",
        "# Ignore errors about the GUI modules not being installed. We don't use the GUI for this anyway    \n",
        "import hyperspy.api as hs\n",
        "\n",
        "# -- Import Tensorflow Packages\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, losses, Input\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.models import save_model, Model\n",
        "from tensorflow.keras.layers import Dropout, Input, Reshape, Activation\n",
        "from tensorflow.keras.layers import BatchNormalization, Dense, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
        "\n",
        "from tensorflow.keras.losses import mse, sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
        "from tensorflow.keras.metrics import BinaryCrossentropy\n",
        "\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "\n",
        "# Set random seeds\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Double check tensorflow install and python version\n",
        "print('Tensorflow version')\n",
        "print(tf.__version__)\n",
        "print(tf.test.is_built_with_cuda())\n",
        "\n",
        "print(\"Python version\")\n",
        "print (sys.version)\n",
        "\n",
        "\n",
        "#%% === FUNCTIONS\n",
        "def readDM42(fname, emin=440., emax=580., path_files=path_files, path_dark=path_dark):\n",
        "    # Input: fname - filename of the dm4 file to load\n",
        "    #        emin - minimum energy for croppig spectrum\n",
        "    #        emin - maximum energy for croppig spectrum\n",
        "    #        path_files - location of dm4 file to load\n",
        "    #        path_dark - full path (inld. filename) to the dark file\n",
        "\n",
        "    # Output: hs_pixels - 400FPS pixels, 560 ebins (440 to 580eV)\n",
        "    #        y_groundtruth - 1FPS ground truth, 560 bin (440 to 580eV)\n",
        "\n",
        "    # --- Load dark file\n",
        "    dark = hs.load(path_dark)\n",
        "    # --- Load DM4 with HyperSpy\n",
        "    hs_template = hs.load(os.path.join(path_files, fname))\n",
        "    hs_pixels =  hs_template + dark\n",
        "    # Rebin the spectrum from 0.125eV to 0.25eV with Hyperspy\n",
        "    hs_pixels = hs_pixels.rebin(scale=[1,1,2], crop=True, out=None)\n",
        "    # crop from 440 to 580 eV\n",
        "    hs_pixels = hs_pixels.isig[emin:emax]\n",
        "    #print(hs_pixels.data.shape)\n",
        "    \n",
        "  \n",
        "    # --- Create Ground Truths @ 1FPS (440 to 580eV)\n",
        "    y_1fps = []\n",
        "    for j in range(16):\n",
        "        section_min = 40*j\n",
        "        section_max = 40*(j+1)\n",
        "        placeholder = hs_pixels.copy().inav[section_min:section_max, 0:10]\n",
        "        placeholder = placeholder.mean(axis=0).mean(axis=0)\n",
        "\n",
        "        # Append to y_train\n",
        "        y_1fps.append(placeholder.data)\n",
        "    y_1fps = np.asarray(y_1fps)\n",
        "    ebins = y_1fps.shape[-1]\n",
        "\n",
        "    # Repeat each spectrum to match w/ corresponding area of pixels in SI\n",
        "    y_groundtruth = []\n",
        "    for s in y_1fps:\n",
        "        spec = np.repeat([s], [400], axis=0).tolist()\n",
        "        y_groundtruth.append(spec)\n",
        "    y_groundtruth = np.asarray(y_groundtruth)\n",
        "\n",
        "    #hs_pixels = hs_pixels.isig[520.:580.]\n",
        "    hs_pixels = np.asarray(hs_pixels.data)\n",
        "    xmax, ymax = hs_pixels.shape[0], hs_pixels.shape[1]\n",
        "    # crop end of ground truth SI with no corresponding pixels\n",
        "    y_groundtruth = y_groundtruth.reshape(xmax, -1, y_groundtruth.shape[-1])\n",
        "    y_groundtruth = y_groundtruth[:, 0:ymax, :]\n",
        "    \n",
        "    # Return:\n",
        "    #y_train: 400FPS, 440 to 580eV\n",
        "    #y_groundtruth: 1FPS 440 to 580eV\n",
        "    return(hs_pixels, y_groundtruth)\n",
        "\n",
        "def fitgaussian(spectra, axis):\n",
        "    mod = GaussianModel()\n",
        "    prepeak_pars = mod.guess(spectra, x=axis)\n",
        "    prepeak_out = mod.fit(spectra, prepeak_pars, x=axis)\n",
        "    return(prepeak_out)\n",
        "\n",
        "def readDM4(fname, emin=520., emax=580., path_files=path_files, path_dark=path_dark):\n",
        "    # Input: fname - filename of the dm4 file to load\n",
        "    #        emin - minimum energy for croppig spectrum\n",
        "    #        emin - maximum energy for croppig spectrum\n",
        "    #        path_files - location of dm4 file to load\n",
        "    #        path_dark - full path (inld. filename) to the dark file\n",
        "\n",
        "    # Output: hs_template - \n",
        "    #        sample_nobackground.data - \n",
        "    #        y_sample - \n",
        "\n",
        "    # --- Load dark file\n",
        "    dark = hs.load(path_dark)\n",
        "    # --- Load DM4 with HyperSpy\n",
        "    hs_template = hs.load(os.path.join(path_files, fname))\n",
        "    hs_template =  hs_template + dark\n",
        "    # Rebin the spectrum from 0.125eV to 0.25eV with Hyperspy\n",
        "    hs_template = hs_template.rebin(scale=[1,1,2], crop=True, out=None)\n",
        "\n",
        "    y_train = []\n",
        "    y_sample = []\n",
        "    # --- Create Ground Truths @ 1FPS\n",
        "    # PowerLaw Denoise the 10x40 1FPS sections \n",
        "    for j in range(16):\n",
        "        section_min = 40*j\n",
        "        section_max = 40*(j+1)\n",
        "        placeholder = hs_template.copy().inav[section_min:section_max, 0:10]\n",
        "        placeholder = placeholder.mean(axis=0).mean(axis=0).remove_background(signal_range=(500.,524.),\n",
        "                          background_type='PowerLaw',\n",
        "                          fast=False,\n",
        "                          return_model=False)\n",
        "\n",
        "        # Crop to Oxygen Peaks range using user specified emin and emax\n",
        "        placeholder = placeholder.isig[emin:emax]\n",
        "        # Append to y_train\n",
        "        y_train.append(placeholder.data)\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    # PowerLaw Denoise the mean of the entire SI\n",
        "    # (Used for making some of the graphics)\n",
        "    si_nobackground = hs_template.copy().sum(axis=0).sum(axis=0).remove_background(signal_range=(500.,524.),\n",
        "                        background_type='PowerLaw',\n",
        "                        fast=False,\n",
        "                        return_model=False)\n",
        "\n",
        "    # Repeat each spectrum to match w/ corresponding area of pixels in SI\n",
        "    for s in y_train:\n",
        "        spec = np.repeat([s], [400], axis=0).tolist()\n",
        "        y_sample.append(spec)\n",
        "    y_sample = np.asarray(y_sample).reshape(-1, 240)\n",
        "    y_sample = np.asarray(y_sample).reshape(10, -1, 240)\n",
        "\n",
        "    # Crop the last 38 that are < 1FPS\n",
        "    y_sample = y_sample.reshape(10, -1, y_train.shape[-1])\n",
        "    y_sample = y_sample[:, 0:638, :]\n",
        "\n",
        "    # keep only portion of SI UPTO the Oxygen peak\n",
        "    # This is needed to correctly denoise later\n",
        "    hs_template = hs_template.isig[440.:emax] \n",
        "    #hs_template = hs_template.inav[0:638, :]\n",
        "\n",
        "    si_nobackground = si_nobackground.isig[emin:emax]\n",
        "\n",
        "    return(hs_template, si_nobackground.data, y_sample)\n",
        "\n",
        "def plot_losses(hist):\n",
        "    # Plot of loss for trained model\n",
        "    plt.title('Model loss')\n",
        "\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "def expand_yvalues(ydata, xmax, ymax):\n",
        "    y_classes = []\n",
        "    for y in ydata:\n",
        "        y_classes.extend([y] * (xmax*ymax))\n",
        "    y_classes = np.asarray(y_classes)\n",
        "    return(y_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lmfit in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: asteval>=0.9.16 in /usr/local/lib/python3.7/dist-packages (from lmfit) (0.9.23)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from lmfit) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from lmfit) (1.4.1)\n",
            "Requirement already satisfied: uncertainties>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from lmfit) (3.1.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from uncertainties>=3.0.1->lmfit) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Tensorflow version\n",
            "2.4.1\n",
            "True\n",
            "Python version\n",
            "3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Uqpv9aIt8T"
      },
      "source": [
        "#%% === LOAD TRAINING ===\n",
        "# Note: The new version of Hyperspy gives an error about Axis calibration mismatch\n",
        "# This can be safely ignored for now until the code can be updated to not throw the error message\n",
        "# --- Read DM4 Names ---\n",
        "# get filenames\n",
        "exsitu_filenames = [f for f in listdir(path_files) if isfile(join(path_files, f))]\n",
        "\n",
        "# Specify which samples to keep for the test data set\n",
        "exsitu_filenames_test = ['EELS thin 1_1.dm4', 'EELS thick 1_1.dm4',\n",
        "                         'EELS thin 1_1a.dm4', 'EELS thick 1_1a.dm4']\n",
        "\n",
        "# Get names of the training data set\n",
        "exsitu_filenames_train = [f for f in exsitu_filenames if f not in exsitu_filenames_test]\n",
        "\n",
        "# Print training filenames for referencing later in graphs\n",
        "for i in np.arange(len(exsitu_filenames_train)/2):\n",
        "    print('{} - {}     {} - {}'.format(int(i), exsitu_filenames_train[int(i)],\n",
        "                                       int(i+10),\n",
        "                                       exsitu_filenames_train[int(i)+10]))\n",
        "# Read dm4 files\n",
        "X_train = []\n",
        "y_train = []\n",
        "start = time.process_time()\n",
        "for fname in exsitu_filenames_train:\n",
        "    print('loading: {}'.format(fname))\n",
        "    sample, sample_groundtruth = readDM42(fname, emin=440., emax=580., path_files=path_files, path_dark=path_dark)\n",
        "\n",
        "    X_train.append(sample) #raw 400FPS spectral data, 560 bins long 440-580eV\n",
        "    y_train.append(sample_groundtruth) #raw 1FPS Ground Truth 560 bins long 440-580eV\n",
        "\n",
        "\n",
        "# Process/trim data after loaded\n",
        "y_train = np.asarray(y_train) #raw 1FPS Ground Truth 560 bins long 440-580eV\n",
        "X_train = np.asarray(X_train)[:, :, :, -240:] #400FPS spectral data, 240 bins long 520-580eV\n",
        "xmax, ymax = X_train.shape[1], X_train.shape[2]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "end = time.process_time() - start\n",
        "print('Runtime is {} sec to load {} files'.format(end, X_train.shape[0]))\n",
        "\n",
        "# BACKGROUND SUBTRACT Y_TRAIN\n",
        "# eV range for background subtraction\n",
        "eV_increment = 0.25\n",
        "emin = 440\n",
        "axes = np.arange(0,np.shape(y_train)[-1])*0.25+emin\n",
        "\n",
        "index_min = int((498 - emin) / eV_increment)\n",
        "index_max = int((528 - emin) / eV_increment)\n",
        "x = axes[index_min:index_max]\n",
        "\n",
        "# Fit powerlaw\n",
        "start = time.process_time()\n",
        "\n",
        "y_train = y_train.reshape(-1, y_train.shape[-1])\n",
        "y_train_nobg = []\n",
        "counter = 0\n",
        "for counter, s in enumerate(y_train):\n",
        "    # Fit Powerlaw model to before pre-peak\n",
        "    fit_region = s.copy()[index_min:index_max]\n",
        "\n",
        "    # Model\n",
        "    mod = PowerLawModel()\n",
        "    pars = mod.guess(fit_region, x=x)\n",
        "    out = mod.fit(fit_region, pars, x=x)\n",
        "    #Powerlaw subtract y=A*x*exp(-r)\n",
        "    A = out.best_values['amplitude']\n",
        "    r = out.best_values['exponent']\n",
        "\n",
        "    # Apply fit to sample\n",
        "    fx = A * np.power(axes, r)\n",
        "    y_train_nobg.append(s - fx)\n",
        "    if counter % 10000 == 0:\n",
        "        print('...Finished fitting {} out of {} spectra'.format(counter, y_train.shape[0]))\n",
        "        \n",
        "y_train = np.array(y_train_nobg.copy()) #convert from list to arrayx_max = int((528 - emin) / eV_increment)\n",
        "\n",
        "\n",
        "# eV range for y_test (520 to 580eV)\n",
        "index_min = int((520 - emin) / eV_increment)\n",
        "index_max = int((580 - emin) / eV_increment)\n",
        "y_train = y_train[:, index_min:index_max]\n",
        "y_train = y_train.reshape(len(exsitu_filenames_train), -1, y_train.shape[-1])\n",
        "\n",
        "end = time.process_time()\n",
        "print('Runtime is {} for background subtracat on batch of shape {}'.format(end-start, y_train.shape))\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1, np.shape(X_train)[-1])\n",
        "y_train = y_train.reshape(-1, np.shape(y_train)[-1])\n",
        "\n",
        "# Robust Scaling\n",
        "robust_scaler = RobustScaler(quantile_range=(0.25, 0.75)).fit(X_train.copy())\n",
        "X_train = robust_scaler.fit_transform(X_train)\n",
        "X_train_scaled = X_train.copy()\n",
        "\n",
        "# Random Noise (Option 1)\n",
        "noise_factor = 0.4\n",
        "X_train = X_train.copy() + noise_factor * tf.random.normal(shape=X_train.shape) \n",
        "X_train = X_train + noise_factor * np.random.normal(size=X_train.shape)\n",
        "\n",
        "# Poisson Noise (Option 2)\n",
        "#noisy_samples = []\n",
        "#for i, sample in enumerate(X_train):\n",
        "#  noise_mask = np.random.poisson(sample)\n",
        "#  noisy_samples.append(sample + noise_mask)\n",
        "#X_train = np.array(noisy_samples)\n",
        "\n",
        "# Make 3D for correct input to NNs\n",
        "X_train = np.atleast_3d(X_train).astype('float32')\n",
        "y_train = np.atleast_3d(y_train).astype('float32')\n",
        "\n",
        "print('Making binary and quad classes...')\n",
        "# --- Make Binary Classes - Green v. Annealed\n",
        "filename_binary_train = np.where(~np.char.endswith(exsitu_filenames_train, 'a.dm4'),\n",
        "                           exsitu_filenames_train, 1)\n",
        "filename_binary_train = np.where(~np.char.endswith(filename_binary_train, '.dm4'),\n",
        "                           filename_binary_train, 0)\n",
        "# Expand the [0,1] values from sample name to each pixel - 400FPS\n",
        "y_binary_train = expand_yvalues(filename_binary_train, xmax=xmax, ymax=ymax)\n",
        "y_binary_train = y_binary_train.astype('int32')\n",
        "\n",
        "# Encode classes for training in Keras\n",
        "# Binary class\n",
        "y_binary_train = np.atleast_3d(tf.keras.utils.to_categorical(y_binary_train, num_classes=2))\n",
        "print(y_binary_train.shape)\n",
        "print(X_train.shape[0]*X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkSlbBY_zGeZ",
        "outputId": "25195a0c-ef05-4a9c-bca7-848a316782e6"
      },
      "source": [
        "#%% === TRAIN AUTOENCODER ===\n",
        "def create_CNN(X_train, latent_dim):\n",
        "\n",
        "    # --- MODEL PARAMETERS\n",
        "    latent_dims = 5\n",
        "    batch_size = 512 \n",
        "    epochs = 500 \n",
        "\n",
        "    dropout = 0.2\n",
        "    l1_norm = 3e-4\n",
        "    learning_rate = 1e-4\n",
        "    #loss = 'mse'\n",
        "\n",
        "    # Encoder/Decoder layers and filters per layer (v2)\n",
        "    aec_layer_filters = [8, 16, 16, 32, 64]\n",
        "    aec_layer_kernels = [7, 7, 5, 5, 3]\n",
        "\n",
        "    spectrum_size = X_train.shape[1]\n",
        "    input_shape = (spectrum_size, 1)\n",
        "\n",
        "    # BUILD CNN AEC\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    x = encoder_input\n",
        "    for i, lf in enumerate(aec_layer_filters):\n",
        "        x = Conv1D(filters=lf, kernel_size=aec_layer_kernels[i],\n",
        "                   strides=2,\n",
        "                   padding='same',\n",
        "                   activation='relu',\n",
        "                   name='conv1D{}'.format(i))(x)  \n",
        "        if i < len(aec_layer_filters)-1:\n",
        "            x = Dropout(dropout)(x)\n",
        "\n",
        "    # Shape info needed to build Decoder Model\n",
        "    shape = K.int_shape(x)\n",
        "\n",
        "    # --- Latent Space\n",
        "    x = Flatten(name='flattened')(x)\n",
        "    latent_space = Dense(latent_dim, name='latent_space', activation='relu')(x)\n",
        "\n",
        "    # Reshape input to decoder after Dense layer\n",
        "    x = Dense(shape[1] * shape[2])(latent_space)\n",
        "    x = Reshape((shape[1], shape[2]))(x)     \n",
        "\n",
        "    # --- Decoder\n",
        "    x = Conv1DTranspose(filters=aec_layer_filters[-1],\n",
        "                        kernel_size=aec_layer_kernels[-1],\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        output_padding=1,\n",
        "                        activation='relu')(x)\n",
        "                \n",
        "    for i, lf in enumerate(aec_layer_filters[0:-1][::-1]):\n",
        "        x = Conv1DTranspose(filters=lf, kernel_size=aec_layer_kernels[0:-1][::-1][i],\n",
        "                            strides=2,\n",
        "                            padding='same',\n",
        "                            activation='relu')(x)\n",
        "\n",
        "    # Spectrum\n",
        "    decoded = Conv1DTranspose(filters=1, kernel_size=aec_layer_kernels[0], padding='same',\n",
        "                              activation='linear',\n",
        "                              name='decoded_spectrum')(x) \n",
        "    autoencoder = Model(encoder_input, decoded)\n",
        "\n",
        "    # Compile model\n",
        "    autoencoder.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                        optimizer='adam')\n",
        "    return(autoencoder)\n",
        "\n",
        "# Create Model\n",
        "aec_model = create_CNN(X_train, latent_dims)\n",
        "aec_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 240, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv1D0 (Conv1D)             (None, 120, 8)            48        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 120, 8)            0         \n",
            "_________________________________________________________________\n",
            "conv1D1 (Conv1D)             (None, 60, 16)            656       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 60, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv1D2 (Conv1D)             (None, 30, 32)            2592      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 30, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1D3 (Conv1D)             (None, 15, 64)            10304     \n",
            "_________________________________________________________________\n",
            "flattened (Flatten)          (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "latent_space (Dense)         (None, 5)                 4805      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 960)               5760      \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 15, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_transpose (Conv1DTran (None, 30, 64)            20544     \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_1 (Conv1DTr (None, 60, 32)            10272     \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_2 (Conv1DTr (None, 120, 16)           2576      \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_3 (Conv1DTr (None, 240, 8)            648       \n",
            "_________________________________________________________________\n",
            "decoded_spectrum (Conv1DTran (None, 240, 1)            41        \n",
            "=================================================================\n",
            "Total params: 58,246\n",
            "Trainable params: 58,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2yb8QyI1yh"
      },
      "source": [
        "#%% === TRAINING PARAMETERS ===\n",
        "# Autoencoder\n",
        "aec_batch_size =  512\n",
        "aec_epochs = 500\n",
        "# Classifier\n",
        "cf_batch_size =  512\n",
        "cf_epochs = 500\n",
        "\n",
        "\n",
        "#%% === TRAIN DENOISING AEC ===\n",
        "start_time = datetime.now().strftime(\"%H:%M:%S\")\n",
        "print(\"Start training at Time = {}\".format(start_time))\n",
        "hist = aec_model.fit(X_train, y_train,\n",
        "                     epochs=aec_epochs,\n",
        "                     batch_size=aec_batch_size,\n",
        "                     verbose=1,\n",
        "                     shuffle=True)\n",
        "\n",
        "# --- Save AEC model - Optional ---\n",
        "#aec_path = '/content/drive/MyDrive/Final EELS/Other March Models'\n",
        "#aec_filename = 'aecModel-e{}-bs{}-ld{}-{}-v3'.format(epochs, batch_size, latent_dims, date.today().strftime(\"%b%d\"))\n",
        "#aec_name = join(aec_path, aec_filename)\n",
        "#aec_model.save(aec_name)\n",
        "#print('Model saved to: {}'.format(aec_name))\n",
        "\n",
        "end_time = datetime.now().strftime(\"%H:%M:%S\")\n",
        "print(\"Started training at: {} Ended at: {}\".format(start_time, end_time))\n",
        "\n",
        "#%% === TRAIN CLASSIFIER ===\n",
        "# --- Separate Encoder from AEC Model ---\n",
        "encoder_model = Model(inputs=aec_model.input,\n",
        "                      outputs=aec_model.get_layer('latent_space').output)\n",
        "\n",
        "encoder_model.trainable = False  # Freeze the inner model\n",
        "assert encoder_model.trainable == False  # All layers in model are now frozen\n",
        "assert encoder_model.layers[0].trainable == False  #trainable is propagated recursively\n",
        "encoder_model.compile()\n",
        "\n",
        "# --- Create Classifier\n",
        "classifier_input = Input(shape=(X_train.shape[1], 1), name='classifier_input')\n",
        "x = encoder_model(classifier_input, training=False)\n",
        "\n",
        "classifier_output = Dense(2, activation='softmax')(x) \n",
        "classifier_model = Model(classifier_input, classifier_output)\n",
        "classifier_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                         optimizer='adam',\n",
        "                         metrics=['accuracy', 'categorical_accuracy'])\n",
        "#classifier_model.summary()\n",
        "\n",
        "# --- Train Classifier Model \n",
        "start = time.perf_counter()\n",
        "hist_classifier = classifier_model.fit(X_train, y_binary_train,\n",
        "                                       epochs=cf_epochs,\n",
        "                                       batch_size=cf_batch_size,\n",
        "                                       shuffle=True)\n",
        "\n",
        "# --- Save model (must have GDrive authorized) - Optional\n",
        "#classifier_path = 'tmp/models/'\n",
        "#classifier_filename = 'classifierModel-e{}-bs{}-ld{}-{}-quadclass-v3'.format(epochs, batch_size, latent_dims, date.today().strftime(\"%b%d\"))\n",
        "#classifier_name = join(classifier_path, classifier_filename)\n",
        "#classifier_model.save(classifier_name)\n",
        "#print('Model saved to: {}'.format(classifier_name))\n",
        "\n",
        "end = (time.perf_counter() - start)\n",
        "end_time = datetime.now().strftime(\"%H:%M:%S\")\n",
        "print('Classifier Total training time: {} sec'.format(end))\n",
        "print(\"Started training models at: {} Ended at: {}\".format(start_time, end_time))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}